---
title: "wine data set"
output: html_document
---

### Read Data from url
```{r echo=F, message=F}
library(data.table)
wineData <- fread('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')
colnames(wineData) = c('class','alcohol','malicAcid','ash','alcalinityOfAsh','magnesium','totalPhenols',
                      'flavanoids','nonflavanoidPhenols','proanthocyanins','colorIntensity',
                     'hue','OD280_OD315OfDilutedWines','proline')
head(wineData)
```


```{r results='hide'}
# update class of data.table to data.frame
class(wineData)
wineData = data.frame(wineData)

# convert class to a factor variable
wineData$class = factor(wineData$class)

# remove need for $ expansion
attach(wineData)

```

### (1) Data Quality Check
The data set consists of 178 observations and 14 variables. Of the 14 variables there is one class variable and 13 continuous variables that describe the profile of the wine. 

```{r}
dim(wineData)   # 178 row x 14 columns (13 continuous variables and 1 class variable)
str(wineData)   # 
```


There is a fairly even amount of the three classes of wine the wine data set, however, the larger portion of wines fall into class 2 (40%), followed by class 1 (33%), and then class 3 (27%). This can be seen from the output of Hmisc::describe below. The function also confirms that there are no missing observations in the data set. Based upon the mean and median values the proline variable appears to suffer from a non-normal distribution of values. The mean and median values of the proline variable are noticeable different. The mean value of proline is ... and the median value is ... The proline variable has some potential outlier values. 

```{r}
#library(Hmisc)
Hmisc::describe(wineData)
```

By plotting a matrix of histograms we can visually identify clear outliers in the data set that are described above by the Hmisc:describe function. 
Outliers can be found in the following variables: ash, magnesium, flavanoids, hue. 

Outliers:
i.   left tail of ash
ii.  right tail of ash
iii. right tail of magnesium
iv.  right tail of flavanoids
v.   right tail of hue


```{r}
par(mfrow=c(2,2))
hist(ash,col="blue",main='Frequency of Ash', xlab='ash')
hist(ash,col="blue",main='Frequency of Magnesium', xlab='magnesium')
hist(ash,col="blue",main='Frequency of Flavanoids', xlab='flavanoids')
hist(ash,col="blue",main='Frequency of Hue', xlab='hue')
```


### (2) Exploratory data analysis
With the wine data set we have a classification problem, so let's look at some boxplots to find some interesting relationships in the wine data. From the boxplots below we can see that alcohol, totalPhenols, and flavanoids appear to be particularly predictive with the middle 75% of values breaking out the three wine classes with minimal overlap. 

```{r}
par(mfrow=c(1,3))
boxplot(alcohol~class, data=wineData, col='lightgray',xlab="class", ylab='alcohol')
boxplot(totalPhenols~class, data=wineData, col='lightgray',xlab="class", ylab='totalPhenols')
boxplot(flavanoids~class, data=wineData, col='lightgray',xlab="class", ylab='flavanoids')
```


Several of the variables appear to be good candidates for a descriminative classifier such as a Support Vector Machine (SVM) which use an algorithm to split the data classes into optimal hyperplanes. To see this we plot scatterplot pairs for the variables most likely to perform well by variables. 
```{r}
names(wineData)
wineData = data.frame(wineData)

# pairings that split the wine class into clearly defined hyperplanes with alcohol
pairs(wineData[,c('alcohol','malicAcid','flavanoids','OD280_OD315OfDilutedWines','totalPhenols','proline')], col=wineData$class)

# and with magnesium
pairs(wineData[,c('magnesium','flavanoids','OD280_OD315OfDilutedWines')], col=wineData$class)

# and totalPhenols
pairs(wineData[,c('totalPhenols','flavanoids','colorIntensity','proline')], col=wineData$class)

# Other variable pairs that split the data over defined hyperplanes
# flavanoids: nonflavanoidPhenols, colorIntensity, OD280/OD315OfDilutedWines, proline
# proacnthocyanins: colorIntensity
# colorIntensity: hue, proline
# hue: proline
# OD280/OD315OfDilutedWines:proline
# proline: malicAcid, OD280/OD315OfDilutedWines


```

Let's try to use the Lattice functionality to help determine distribution of each variable. 
```{r}
library('lattice')
histogram(~ alcohol | factor(class), data = wineData)
histogram(~ malicAcid | factor(class), data = wineData)
```

```{r}
densityplot(~ alcohol, data = wineData, groups = class,
plot.points = FALSE, ref = TRUE,
auto.key = list(columns = 3, title='wine class'), main='Density Plot Alcohol by Class')
```

Given that many variable pairs split the data extremely well, let's now use a tree model to explore the data further. 

```{r}
require(tree)
tree.wineData=tree(class~., data=wineData)
summary(tree.wineData)
plot(tree.wineData)
text(tree.wineData, pretty=0) # terminal nodes are 1,2,and 3. 
```

#### (3) Model Based Exploratory Data Analysis
We can fit some ‘naïve’ or exploratory models to help us understand the relationships in our data.
We can call these models ‘naïve’ models because they are models that we fit without much thought.
They are simply some initial exploratory models. What types of models are relevant for this
statistical problem? Can principal components analysis or cluster analysis be useful as an
exploratory tool on this problem?

Fit a tree model and any other models that can be used to help you understand the data.



The classification tree has a misclassification error of 1.7% by using just five of the 13 variables. The five variables are: flavanoids, colorIntensity, malicAcid, proline, and alcohol. 


### Appendix of relevant R code related to the results or graphics

```{r eval=F}

# Loop through all variables to produce a matrix of histograms to identify outliers 
par(mfrow=c(2,2))
for (i in 2:14){
  hist(wineData[[i]],col="blue",main='Histogram', xlab=names(wineData)[i])
} 

# Loop through all variables to create boxplots of each variable against the wine class.
par(mfrow=c(1,3))
for (i in 2:14){
  boxplot(wineData[[i]]~class, data=wineData, col='lightgray',xlab="class", ylab=names(wineData)[i])          
}

# Loop through all variables to produce scatterplots of each variable with class as color variable
for (r in 14:14){    # change 14:14 to 2:2, 3:3, for each non-class variable
  for (i in 2:14){   # loop through each non-class variable
    pairs(wineData[,c(r,i)], col=wineData$class)
  }
}
```

```


From: http://archive.ics.uci.edu/ml/machine-learning-databases/wine/Index
Index of wine

02 Dec 1996      105 Index
30 Oct 1995    10782 wine.data
19 Sep 1992     2643 wine.names


From: http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names
1. Title of Database: Wine recognition data
	Updated Sept 21, 1998 by C.Blake : Added attribute information

2. Sources:
   (a) Forina, M. et al, PARVUS - An Extendible Package for Data
       Exploration, Classification and Correlation. Institute of Pharmaceutical
       and Food Analysis and Technologies, Via Brigata Salerno, 
       16147 Genoa, Italy.

   (b) Stefan Aeberhard, email: stefan@coral.cs.jcu.edu.au
   (c) July 1991
3. Past Usage:

   (1)
   S. Aeberhard, D. Coomans and O. de Vel,
   Comparison of Classifiers in High Dimensional Settings,
   Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of
   Mathematics and Statistics, James Cook University of North Queensland.
   (Also submitted to Technometrics).

   The data was used with many others for comparing various 
   classifiers. The classes are separable, though only RDA 
   has achieved 100% correct classification.
   (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))
   (All results using the leave-one-out technique)

   In a classification context, this is a well posed problem 
   with "well behaved" class structures. A good data set 
   for first testing of a new classifier, but not very 
   challenging.

   (2) 
   S. Aeberhard, D. Coomans and O. de Vel,
   "THE CLASSIFICATION PERFORMANCE OF RDA"
   Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of
   Mathematics and Statistics, James Cook University of North Queensland.
   (Also submitted to Journal of Chemometrics).

   Here, the data was used to illustrate the superior performance of
   the use of a new appreciation function with RDA. 

4. Relevant Information:

   -- These data are the results of a chemical analysis of
      wines grown in the same region in Italy but derived from three
      different cultivars.
      The analysis determined the quantities of 13 constituents
      found in each of the three types of wines. 

   -- I think that the initial data set had around 30 variables, but 
      for some reason I only have the 13 dimensional version. 
      I had a list of what the 30 or so variables were, but a.) 
      I lost it, and b.), I would not know which 13 variables
      are included in the set.

   -- The attributes are (dontated by Riccardo Leardi, 
	riclea@anchem.unige.it )
 	1) Alcohol
 	2) Malic acid
 	3) Ash
	4) Alcalinity of ash  
 	5) Magnesium
	6) Total phenols
 	7) Flavanoids
 	8) Nonflavanoid phenols
 	9) Proanthocyanins
	10)Color intensity
 	11)Hue
 	12)OD280/OD315 of diluted wines
 	13)Proline            

5. Number of Instances

      	class 1 59
	      class 2 71
	      class 3 48

59+71+48
6. Number of Attributes 
	
	13

7. For Each Attribute:

	All attributes are continuous
	
	No statistics available, but suggest to standardise
	variables for certain uses (e.g. for us with classifiers
	which are NOT scale invariant)

	NOTE: 1st attribute is class identifier (1-3)

8. Missing Attribute Values:

	None

9. Class Distribution: number of instances per class

      	class 1 59
	      class 2 71
	      class 3 48
