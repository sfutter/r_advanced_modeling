---
output:
  pdf_document: default
  html_document: default
---

### Assignment #1: Statistical Graphics and Exploratory Data Analysis 
Steven Futter: PRED 454-SEC 55


```{r echo=F, message=F}
### Read Data from url
library(data.table)
wineData <- fread('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')
colnames(wineData) = c('class','alcohol','malicAcid','ash','alcalinityOfAsh','magnesium','totalPhenols',
                      'flavanoids','nonflavanoidPhenols','proanthocyanins','colorIntensity',
                     'hue','OD280_OD315OfDilutedWines','proline')
```

```{r echo=F, message=F}
# update class of data.table to data.frame
wineData = data.frame(wineData)

# convert class to a factor variable
wineData$class = factor(wineData$class)

# remove need for $ expansion
attach(wineData)

```


#### (1) Data Quality Check

The wine data set consists of 178 observations and 14 variables. Of the 14 variables there is one class variable and 13 continuous variables that describe the profile of the wine. In this assignment we are dealing with a classification problem since the goal of the assignment is to predict which class a wine falls into based upon the continuous variables, such as alcohol level, magnesium content, and color intensity. 

##### Missing Data, Data Ranges and Distributions
Of the 178 observations the larger portion of wines belong to class 2 (40%), followed by class 1 (33%), and then class 3 (27%). The my.summary() function below confirms that there are no missing observations in the wine data. Based upon the mean and median values the proline variable appears to have a non-normal distribution sine mean and median values differ relative to other variables in the data set. The mean and median values of the proline variable are noticeable different. The mean value of proline is 746.89 whereas the median value is 673.50. Other variables in the data set appear to have extreme observations appearing as minimum and maximum values. There are no negative minimum values which makes sense and although there are outliers the values appear to be valid at first glance. 

```{r echo=F}
library(plyr)

maxfun <- function(x){round(max(x,na.rm = TRUE),2)} 
minfun <- function(x){round(min(x,na.rm = TRUE),2)}   
quantilefun <- function(x){round(quantile(x,c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)),2)}
meanfun <- function(x){round(mean(x),2)}
varfun <- function(x){round(var(x),2)}
percentMissingfun <- function(x){round(100*(sum(is.na(x))/length(x)),2)}

my.summary = function(df){
  #create rows of summarized values using colwise function from plyr package
  max = colwise(maxfun)(df)
  min = colwise(minfun)(df)
  quantile = colwise(quantilefun)(df)
  mean = colwise(meanfun)(df)
  variance = colwise(varfun)(df)
  percentMissing = colwise(percentMissingfun)(df)
  
  # bind data.frame and rename rows
  mydf = rbind(max, min, quantile, mean, variance, percentMissing)
  rownames(mydf) = c('max','min',0.01,0.05,0.25,0.5,0.75,0.95,0.99,'mean','variance','% missing')
  return(mydf)
}
```

```{r}
my.summary(wineData[,c(2:14)])
```

By plotting a matrix of histograms we can visually identify clear outliers in the data set that are described above by the my.summary() function. Outliers can be seen in the following variables: ash, magnesium, flavanoids, hue. 

Outliers:
i.   left tail of ash
ii.  right tail of ash
iii. right tail of magnesium
iv.  right tail of flavanoids
v.   right tail of hue


##### Outliers
```{r}
par(mfrow=c(2,2))
hist(ash,col="blue",main='Frequency of Ash', xlab='ash')
hist(magnesium,col="blue",main='Frequency of Magnesium', xlab='magnesium')
hist(flavanoids,col="blue",main='Frequency of Flavanoids', xlab='flavanoids')
hist(hue,col="blue",main='Frequency of Hue', xlab='hue')
```


#### (2) Exploratory data analysis
With the wine data set we have a classification problem which can benefit from viewing the data distributions with boxplots. From the boxplots below we can see that alcohol, totalPhenols, and flavanoids appear to be particularly predictive with the middle 75% (grey shaded bar) of values separating the three wine classes with minimal overlap. This looks promising since the wine classes are differentiated quite nicely by these three variables alone. 

```{r}
par(mfrow=c(1,3))
boxplot(alcohol~class, data=wineData, col='lightgray',xlab="class", ylab='alcohol')
boxplot(totalPhenols~class, data=wineData, col='lightgray',xlab="class", ylab='totalPhenols')
boxplot(flavanoids~class, data=wineData, col='lightgray',xlab="class", ylab='flavanoids')
```


Several of the variables appear to be good candidates for a descriminative classifier such as a Support Vector Machine (SVM). An SVM uses an algorithm to split the data classes into optimal hyperplanes. Below I show that various pairs of variables cluster the wine classes into more or less separate wine classes. From initial EDA it appears that a clustered algorithm approach to modeling may also have a low misclassification error percentage. 

Below we can see how magnesium, flavanoids, and OD280_OD315OfDilutedWines pairs segregate the three wine classes into separate clusters.

```{r echo=F, message=F}
# pairings that split the wine class into clearly defined hyperplanes with alcohol
# pairs(wineData[,c('alcohol','malicAcid','flavanoids','OD280_OD315OfDilutedWines','totalPhenols','proline')], 
# col=wineData$class)
```

```{r}
pairs(wineData[,c('magnesium','flavanoids','OD280_OD315OfDilutedWines')], col=wineData$class)
```

```{r echo=F}
# and totalPhenols
# pairs(wineData[,c('totalPhenols','flavanoids','colorIntensity','proline')], col=wineData$class)

# Other variable pairs that split the data over defined hyperplanes
# flavanoids: nonflavanoidPhenols, colorIntensity, OD280/OD315OfDilutedWines, proline
# proacnthocyanins: colorIntensity
# colorIntensity: hue, proline
# hue: proline
# OD280/OD315OfDilutedWines:proline
# proline: malicAcid, OD280/OD315OfDilutedWines


```

```{r echo=F, message=F}
#library('lattice')
#histogram(~ alcohol | factor(class), data = wineData)
#histogram(~ malicAcid | factor(class), data = wineData)
```

Using the Lattice library we can see the density function of each wine class against alcohol. We can see that the peak densities of each wine class vary as alcohol concentration increases. Generally speaking we can see that class 2 wines have the least alcohol content and class 1 wines have the most. 
```{r message=F}
library('lattice')
densityplot(~ alcohol, data = wineData, groups = class,
plot.points = FALSE, ref = TRUE,
auto.key = list(columns = 3, title='Wine Class'), main='Density Plot Alcohol by Class')
```

I now use a tree model to explore the data further and evaluate missclassification error. 

##### Tree Model 
```{r echo=F, message=F}
require(tree)
tree.wineData=tree(class~., data=wineData)
```

```{r}
summary(tree.wineData)
plot(tree.wineData)
text(tree.wineData, pretty=0) # terminal nodes are 1,2,and 3. 
```

The tree model above has a misclassification of 1.7%. 


#### (3) Model Based Exploratory Data Analysis

In this next section I fit some naive models to help understand the relationships in the data. As can be seen from the tree model above the data can be segregated quite accurately using a tree model. The tree model above has a misclassification error of 1.7% by evaluating the values of just five (flavanoids, colorIntensity, malicAcid, proline, and alcohol) of the 13 variables. Notably the flavanoids variable is the root of the tree with a split value of 1.575. 

##### Discussion of Other Modeling Approaches
As mentioned in the EDA an SVM model should perform well on the wineData.  
 
```{r echo=F}
# Divide wineData to x (the variables) and y the classes
x = subset(wineData, select=-class)
y = class
 
#Create SVM Model and show summary
library(e1071)
svmModel = svm(x,y, data=wineData)
summary(svmModel)

# Run the prediction:
pred = predict(svmModel, x)
```

See the confusion matrix result of prediction:
```{r}
table(pred,y)
```

Looking at the results from the confusion matrix for the SVM model every observation has been correctly classified. Further work is needed to ensure that these results can be relied upon and that the SVM model is not overfitting to the data. However, these results are very pleasing. For further investigation I can review other clustered models such as k-nearest neighbor. Although k-nearest neighbor is an unsupervised learning model it could still be used to evaluate the expected class output in the wine data set.  


### Appendix 

Below I present relevant R code related to the results and graphics above.

```{r eval=F}
# Read in data into data.frame. check first 5 rows.
library(data.table)
wineData <- fread('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')
colnames(wineData) = c('class','alcohol','malicAcid','ash','alcalinityOfAsh','magnesium','totalPhenols',
                      'flavanoids','nonflavanoidPhenols','proanthocyanins','colorIntensity',
                     'hue','OD280_OD315OfDilutedWines','proline')

head(wineData,5) 
dim(wineData)     # 178 row x 14 columns (13 continuous variables and 1 class variable)
str(wineData)     # evaluate the data type of each column


# update class of data.table to data.frame
class(wineData)
wineData = data.frame(wineData)

# convert class to a factor variable
wineData$class = factor(wineData$class)

# remove need for $ expansion
attach(wineData)



#### (1) Data Quality Check
# DATA QUALITY SECTION
# summary function:
library(plyr)

maxfun <- function(x){round(max(x,na.rm = TRUE),2)} 
minfun <- function(x){round(min(x,na.rm = TRUE),2)}   
quantilefun <- function(x){round(quantile(x,c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)),2)}
meanfun <- function(x){round(mean(x),2)}
varfun <- function(x){round(var(x),2)}
percentMissingfun <- function(x){round(100*(sum(is.na(x))/length(x)),2)}

my.summary = function(df){
  #create rows of summarized values using colwise function from plyr package
  max = colwise(maxfun)(df)
  min = colwise(minfun)(df)
  quantile = colwise(quantilefun)(df)
  mean = colwise(meanfun)(df)
  variance = colwise(varfun)(df)
  percentMissing = colwise(percentMissingfun)(df)
  
  # bind data.frame and rename rows
  mydf = rbind(max, min, quantile, mean, variance, percentMissing)
  rownames(mydf) = c('max','min',0.01,0.05,0.25,0.5,0.75,0.95,0.99,'mean','variance','% missing')
  return(mydf)


# Loop through all variables to produce a matrix of histograms to identify outliers 
par(mfrow=c(2,2))
for (i in 2:14){
  hist(wineData[[i]],col="blue",main='Histogram', xlab=names(wineData)[i])
} 

# Loop through all variables to create boxplots of each variable against the wine class.
par(mfrow=c(1,3))
for (i in 2:14){
  boxplot(wineData[[i]]~class, data=wineData, col='lightgray',xlab="class", ylab=names(wineData)[i])          
}

# Loop through all variables to produce scatterplots of each variable with class as color variable
for (r in 14:14){    # change 14:14 to 2:2, 3:3, for each non-class variable
  for (i in 2:14){   # loop through each non-class variable
    pairs(wineData[,c(r,i)], col=wineData$class)
  }
}


#### (2) Exploratory data analysis
require(tree)
tree.wineData=tree(class~., data=wineData)
summary(tree.wineData)
plot(tree.wineData)
text(tree.wineData, pretty=0) # terminal nodes are 1,2,and 3. 


#### (3) Model Based Exploratory Data Analysis
# SVM MODEL 
# Divide wineData to x (the variables) and y the classes
x = subset(wineData, select=-class)
y = class

# Create SVM Model and show summary
library(e1071)
svmModel = svm(x,y, data=wineData)
summary(svmModel)

# Run the prediction:
pred = predict(svmModel, x)

#See the confusion matrix result of prediction:
table(pred,y)

```


