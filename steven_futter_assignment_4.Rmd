---
title: 'Predictive Modeling in Multiclass Classification'
author: "Steven Futter"
date: "2/13/2017"
output: html_document
---


In this paper I fit a model suite of the following models: Random Forest, a Support Vector Machine, and a Neural Network model. The data is the ‘Wine’ data set that can be downloaded from the [Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Wine). Since this is a small data set I only focus on fitting models in-sample. 

```{r echo=F, message=F}
### Read Data from url
library(nnet)
library(randomForest)
library(e1071)
library(data.table)
wineData <- fread('http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data')
colnames(wineData) = c('class','alcohol','malicAcid','ash','alcalinityOfAsh','magnesium','totalPhenols',
                      'flavanoids','nonflavanoidPhenols','proanthocyanins','colorIntensity',
                     'hue','OD280_OD315OfDilutedWines','proline')
```

```{r echo=F, message=F}
# update class of data.table to data.frame
wineData = data.frame(wineData)

# convert class to a factor variable
wineData$class = factor(wineData$class)

# remove need for $ expansion
attach(wineData)

```


#### (1) Data Quality Check

The wine data set consists of 178 observations and 14 variables. Of the 14 variables there is one class variable and 13 continuous variables that describe the profile of the wine. In this assignment we are dealing with a classification problem since the goal of the assignment is to predict which class a wine falls into based upon the continuous variables, such as alcohol level, magnesium content, and color intensity. 

##### Missing Data, Data Ranges and Distributions
Of the 178 observations the larger portion of wines belong to class 2 (40%), followed by class 1 (33%), and then class 3 (27%). The my.summary() function below confirms that there are no missing observations in the wine data. Based upon the mean and median values the proline variable appears to have a non-normal distribution sine mean and median values differ relative to other variables in the data set. The mean and median values of the proline variable are noticeable different. The mean value of proline is 746.89 whereas the median value is 673.50. Other variables in the data set appear to have extreme observations appearing as minimum and maximum values. There are no negative minimum values which makes sense and although there are outliers the values appear to be valid at first glance. 

```{r echo=F}
library(plyr)

maxfun <- function(x){round(max(x,na.rm = TRUE),2)} 
minfun <- function(x){round(min(x,na.rm = TRUE),2)}   
quantilefun <- function(x){round(quantile(x,c(0.01,0.05,0.25,0.5,0.75,0.95,0.99)),2)}
meanfun <- function(x){round(mean(x),2)}
varfun <- function(x){round(var(x),2)}
percentMissingfun <- function(x){round(100*(sum(is.na(x))/length(x)),2)}

my.summary = function(df){
  #create rows of summarized values using colwise function from plyr package
  max = colwise(maxfun)(df)
  min = colwise(minfun)(df)
  quantile = colwise(quantilefun)(df)
  mean = colwise(meanfun)(df)
  variance = colwise(varfun)(df)
  percentMissing = colwise(percentMissingfun)(df)
  
  # bind data.frame and rename rows
  mydf = rbind(max, min, quantile, mean, variance, percentMissing)
  rownames(mydf) = c('max','min',0.01,0.05,0.25,0.5,0.75,0.95,0.99,'mean','variance','% missing')
  return(mydf)
}
```

```{r fig.align='center'}
my.summary(wineData[,c(2:14)])
```

By plotting a matrix of histograms we can visually identify clear outliers in the data set that are described above by the my.summary() function. Outliers can be seen in the following variables: ash, magnesium, flavanoids, hue. 

Outliers:
i.   left tail of ash
ii.  right tail of ash
iii. right tail of magnesium
iv.  right tail of flavanoids
v.   right tail of hue


##### Outliers
```{r echo=F, fig.align='center'}
par(mfrow=c(2,2))
hist(ash,col="blue",main='Frequency of Ash', xlab='ash')
hist(magnesium,col="blue",main='Frequency of Magnesium', xlab='magnesium')
hist(flavanoids,col="blue",main='Frequency of Flavanoids', xlab='flavanoids')
hist(hue,col="blue",main='Frequency of Hue', xlab='hue')
```


#### (2) Exploratory Data Analysis
With the wine data set we have a classification problem which can benefit from viewing the data distributions with boxplots. From the boxplots below we can see that alcohol, totalPhenols, and flavanoids appear to be particularly predictive with the middle 75% (grey shaded bar) of values separating the three wine classes with minimal overlap. This looks promising since the wine classes are differentiated quite nicely by these three variables alone. 

##### Boxplots: Alcohol, totalPhenols, and flavanoids differentiated across wine classes
```{r echo=F, fig.align='center'}
par(mfrow=c(1,3))
boxplot(alcohol~class, data=wineData, col='lightgray',xlab="class", ylab='alcohol')
boxplot(totalPhenols~class, data=wineData, col='lightgray',xlab="class", ylab='totalPhenols')
boxplot(flavanoids~class, data=wineData, col='lightgray',xlab="class", ylab='flavanoids')
```


Several of the variables appear to be good candidates for a descriminative classifier such as a Support Vector Machine (SVM). An SVM uses an algorithm to split the data classes into optimal hyperplanes. Below I show that various pairs of variables cluster the wine classes into more or less separate wine classes. From initial EDA it appears that a clustered algorithm approach to modeling may also have a low misclassification error percentage. Below we can see how magnesium, flavanoids, and OD280_OD315OfDilutedWines pairs segregate the three wine classes into separate clusters.

##### Scatterplots: magnesium, flavanoids, and OD280_OD315OfDilutedWines pairs segregate the wine classes

```{r echo=F, fig.align='center'}
pairs(wineData[,c('magnesium','flavanoids','OD280_OD315OfDilutedWines')], col=wineData$class)
```

Using the Lattice library we can see the density function of each wine class against alcohol. We can see that the peak densities of each wine class vary as alcohol concentration increases. Generally speaking we can see that class 2 wines have the least alcohol content and class 1 wines have the most. 
```{r echo=F, message=F, fig.align='center'}
library('lattice')
densityplot(~ alcohol, data = wineData, groups = class,
plot.points = FALSE, ref = TRUE,
auto.key = list(columns = 3, title='Wine Class'), main='Density Plot Alcohol by Class')
```

I now use a tree model to explore the data further and evaluate missclassification error. 

##### Tree Model 
```{r echo=F, message=F}
require(tree)
tree.wineData=tree(class~., data=wineData)
```

```{r echo=F, message=F, fig.align='center'}
summary(tree.wineData)
plot(tree.wineData)
text(tree.wineData, pretty=0) # terminal nodes are 1,2,and 3. 
```

The tree model above has a misclassification of 1.7%. flavanoids, proline, colorIntensity, malicAcid, and alcohol are important variables in determining the wine's class. Wine class '3' is segregated along the left branch with flavanoid values less than 1.575 and color intensity greater than 3.825. Class 1 and 2 are split either side of the first tree root. 



#### Fitted Model Suite: 
__1. Random Forest__

Firstly, let's create a random forest model taking in all variables within the wine data set. I write them out in the model to make it explicit which variables are being included into the random forest model. 

```{r echo=FALSE, comment=NA, tidy=TRUE}
set.seed(1)
randomForest.fit = randomForest(class~
    alcohol + malicAcid + ash + alcalinityOfAsh +
    magnesium + totalPhenols + flavanoids + nonflavanoidPhenols + proanthocyanins +
    colorIntensity + hue + OD280_OD315OfDilutedWines + proline, 
    data=wineData, mtry=2, importance=TRUE)
randomForest.fit
```

0% missclassification can be seen in the confusion matrix table.  
```{r echo=FALSE, comment=NA, tidy=TRUE}
randomForest.pred = predict(randomForest.fit, wineData)
table(randomForest.pred, wineData$class)
randomForest.error = 1 - mean(randomForest.pred==wineData$class)    
#randomForest.error                                                  # 0% error
```

__2. Support Vector Machine__

Let's now run a SVM model and evaluate the confusion matrix below. Similarly to the Random Forest model the SVM produces a 0% misclassification error. See the confusion matrix table below. 
```{r echo=FALSE, comment=NA, tidy=TRUE}
x = subset(wineData, select=-class)   # Divide wineData to x (the variables) and y the classes
y = wineData$class

svm.fit = svm(x,y,data=wineData)    # Create SVM model
summary(svm.fit)

svm.pred=predict(svm.fit, x)
table(svm.pred, y)
svm.error = 1 - mean(svm.pred==y)  
#svm.error                     # 0% error

```


__3. Neural Network Model__

Finally, I run a neural network model using the nnet library. Again, I produce a confusion matrix to evaluate the model's performance. 
```{r echo=FALSE, comment=NA, message=F, tidy=TRUE}
#The neural network requires that the species be normalized using one-of-n normalization. We will normalize between 0 and 1. This can be done with the following command.
ideal <- class.ind(wineData$class)

# Train the neural network
set.seed(123)
wineANN = nnet(wineData[,-1], ideal, size=25, softmax=TRUE)

# test the output from the neural network on the validation data
pred = predict(wineANN, wineData[,-1], type="class")

# confusion matrix
table(wineData[,1], pred)

# misclassification error
mean(pred==wineData$class) # 0.9775280899 mean accuracy
nn.error = 1 - mean(pred==wineData$class) 
#nn.error*100        # 2.25% error
```

A plot of a neural network can be produced using the plot.nnet function, however, it's output is best to demonstrate that 25 nodes were created in the process. These nodes can be seen in the middle of the plot below. 

##### Neural Network Plot: 1 layer separating variable inputs and class outputs

The plot below highlights that different weights are added to each model input to produce the neural network model's class output prediction. 
```{r echo=FALSE, comment=NA, message=F, tidy=TRUE, fig.align='center'}
#install.packages('devtools')
#install.packages('reshape')
library(devtools)
require(reshape)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
 
#plot each model
plot.nnet(wineANN)
```

##### Results
Since each model is a 'black box' when it comes to determining how the inputs are treated, it is important that each model is compared for predictive accuracy. To compare each model I present the output table below which provides the misclassification error rate for each model. As the assignment asked to evaluate results in-sample I provide the misclassification error rate for each model using the entire set of wine data observations.  

```{r echo=FALSE, message=FALSE, fig.align='center', comment=NA, tidy=TRUE}
model = c('Random Forest', 'Support Vector Machine', 'Neural Network')
model.errors = c(randomForest.error, svm.error, nn.error)
results = data.frame(model, model.errors)
results
```

As can be seen from the results table the Random Forest and SVM models have excellent results --- both have a 0% misclassification error. The Neural Network model did not perform as well, but its misclassification error was still very low with. Misclassification error for the Neural Network model is  2.25%. 


#### Appendix
```{r eval=F}

# 1. Random Forest
set.seed(1)
randomForest.fit = randomForest(class~
    alcohol + malicAcid + ash + alcalinityOfAsh +
    magnesium + totalPhenols + flavanoids + nonflavanoidPhenols + proanthocyanins +
    colorIntensity + hue + OD280_OD315OfDilutedWines + proline, 
    data=wineData, mtry=2, importance=TRUE)
randomForest.fit


# 2. SVM
x = subset(wineData, select=-class)   # Divide wineData to x (the variables) and y the classes
y = wineData$class

svm.fit = svm(x,y,data=wineData)    # Create SVM model
summary(svm.fit)

svm.pred=predict(svm.fit, x)
table(svm.pred, y)
svm.train.error = 1 - mean(svm.pred==y)  
svm.train.error 


# 3. Neural Net
#The neural network requires that the species be normalized using one-of-n normalization. We will normalize between 0 and 1. This can be done with the following command.
ideal <- class.ind(wineData$class)

# Train the neural network
set.seed(123)
wineANN = nnet(wineData[,-1], ideal, size=25, softmax=TRUE)

# test the output from the neural network on the validation data
pred = predict(wineANN, wineData[,-1], type="class")
table(wineData[,1], pred)

# Plot tutorial found here: https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/
# Plot the neural net output
install.packages('devtools')
install.packages('reshape')
library(devtools)
require(reshape)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
 
#plot the model
plot.nnet(wineANN)
```



